{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# ü¶ú RAG System Tutorial with LangChain\n",
    "\n",
    "This notebook demonstrates how to build a **Retrieval-Augmented Generation (RAG)** system step by step.\n",
    "\n",
    "## What is RAG?\n",
    "RAG combines:\n",
    "- **Retrieval**: Finding relevant documents from a knowledge base\n",
    "- **Generation**: Using an LLM to generate answers based on retrieved context\n",
    "\n",
    "## Architecture Overview\n",
    "```\n",
    "Documents ‚Üí Text Splitting ‚Üí Embeddings ‚Üí Vector DB ‚Üí Retrieval ‚Üí LLM ‚Üí Answer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Install and Import Dependencies\n",
    "\n",
    "First, let's install all required packages and import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this if not already installed)\n",
    "# !pip install langchain langchain-openai langchain-community chromadb beautifulsoup4 requests python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All dependencies imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## üîë Step 2: Configure API Keys and Models\n",
    "\n",
    "We need API keys for:\n",
    "- **OpenAI**: For embeddings (converting text to vectors)\n",
    "- **E2E Networks**: For the language model (generating responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "E2E_LLM_ENDPOINT = os.getenv(\"E2E_LLM_ENDPOINT\")\n",
    "E2E_API_KEY = os.getenv(\"E2E_API_KEY\")\n",
    "E2E_MODEL_NAME = os.getenv(\"E2E_MODEL_NAME\", \"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "\n",
    "print(f\"üìä Using E2E Model: {E2E_MODEL_NAME}\")\n",
    "print(f\"üîó E2E Endpoint: {E2E_LLM_ENDPOINT[:50]}...\" if E2E_LLM_ENDPOINT else \"‚ùå No E2E endpoint configured\")\n",
    "print(f\"üîë OpenAI Key: {'‚úÖ Configured' if OPENAI_API_KEY else '‚ùå Missing'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## üåê Step 3: Document Loading from Websites\n",
    "\n",
    "Let's load documents from LangChain's official documentation websites.\n",
    "\n",
    "### What happens here:\n",
    "1. **Web Scraping**: Download HTML content from URLs\n",
    "2. **Text Extraction**: Extract clean text using BeautifulSoup\n",
    "3. **Document Creation**: Convert to LangChain Document objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_urls(urls):\n",
    "    \"\"\"Load documents from a list of URLs\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    print(\"üìö Loading documents from websites...\")\n",
    "    \n",
    "    for url in urls:\n",
    "        try:\n",
    "            # Step 1: Download the webpage\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Step 2: Parse HTML and extract text\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Remove script and style elements\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose()\n",
    "            \n",
    "            # Get clean text\n",
    "            content = soup.get_text()\n",
    "            content = ' '.join(content.split())  # Clean whitespace\n",
    "            \n",
    "            # Step 3: Create LangChain Document\n",
    "            if len(content) > 200:  # Only save if content is substantial\n",
    "                doc = Document(\n",
    "                    page_content=content,\n",
    "                    metadata={\n",
    "                        \"source\": url,\n",
    "                        \"title\": soup.title.string if soup.title else \"LangChain Documentation\"\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "                print(f\"‚úÖ Loaded: {url}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {url}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"üìñ Successfully loaded {len(documents)} documents\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": "# URLs to load (LangChain documentation)\ndocs_urls = [\n    \"https://python.langchain.com/docs/introduction/\",\n    \"https://python.langchain.com/docs/concepts/vectorstores/\",\n    \"https://python.langchain.com/docs/concepts/text_splitters/\",\n    \"https://python.langchain.com/docs/concepts/embedding_models/\"\n]\n\n# Load the documents\ndocuments = load_documents_from_urls(docs_urls)\n\n# Show what we loaded\nprint(f\"\\nüìÑ Document Summary:\")\nfor i, doc in enumerate(documents[:2]):  # Show first 2\n    print(f\"Document {i+1}:\")\n    print(f\"  Source: {doc.metadata['source']}\")\n    print(f\"  Title: {doc.metadata['title']}\")\n    print(f\"  Content length: {len(doc.page_content)} characters\")\n    print(f\"  Preview: {doc.page_content[:200]}...\\n\")"
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Step 4: Text Splitting\n",
    "\n",
    "Large documents need to be split into smaller chunks because:\n",
    "- **LLM Context Limits**: Models have maximum input lengths\n",
    "- **Better Retrieval**: Smaller chunks = more precise matches\n",
    "- **Efficient Processing**: Faster embedding and search\n",
    "\n",
    "### Parameters:\n",
    "- **chunk_size**: Maximum characters per chunk\n",
    "- **chunk_overlap**: Characters to overlap between chunks (maintains context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,        # Maximum 1000 characters per chunk\n",
    "    chunk_overlap=200,      # 200 characters overlap between chunks\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Split on paragraphs, then lines, then words\n",
    ")\n",
    "\n",
    "print(\"‚úÇÔ∏è Splitting documents into chunks...\")\n",
    "\n",
    "# Split the documents\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"üìÑ Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nüîç Example chunks:\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"  Length: {len(chunk.page_content)} characters\")\n",
    "    print(f\"  Source: {chunk.metadata['source']}\")\n",
    "    print(f\"  Content: {chunk.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## üßÆ Step 5: Embeddings - Converting Text to Vectors\n",
    "\n",
    "**Embeddings** convert text into numerical vectors that capture semantic meaning.\n",
    "\n",
    "### Why Embeddings?\n",
    "- **Semantic Search**: Find documents by meaning, not just keywords\n",
    "- **Mathematical Operations**: Compare similarity using vector math\n",
    "- **Efficient Storage**: Vectors can be indexed for fast search\n",
    "\n",
    "### How it works:\n",
    "```\n",
    "\"What is LangChain?\" ‚Üí [0.1, 0.8, -0.3, 0.5, ...] (1536 dimensions)\n",
    "\"LangChain tutorial\"  ‚Üí [0.2, 0.7, -0.2, 0.6, ...] (similar vector)\n",
    "\"Pizza recipe\"        ‚Üí [0.9, 0.1, 0.8, -0.4, ...] (very different vector)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model=\"text-embedding-ada-002\"  # OpenAI's embedding model\n",
    ")\n",
    "\n",
    "print(\"üßÆ Testing embeddings...\")\n",
    "\n",
    "# Test embedding a simple sentence\n",
    "test_text = \"What is LangChain?\"\n",
    "test_embedding = embeddings.embed_query(test_text)\n",
    "\n",
    "print(f\"üìù Text: '{test_text}'\")\n",
    "print(f\"üî¢ Embedding dimensions: {len(test_embedding)}\")\n",
    "print(f\"üéØ First 5 values: {test_embedding[:5]}\")\n",
    "print(f\"üìä Vector magnitude: {sum(x*x for x in test_embedding)**0.5:.4f}\")\n",
    "\n",
    "# Test similarity between different texts\n",
    "texts = [\n",
    "    \"What is LangChain?\",\n",
    "    \"LangChain tutorial and guide\",\n",
    "    \"How to cook pasta\"\n",
    "]\n",
    "\n",
    "print(\"\\nüîç Testing semantic similarity:\")\n",
    "embeddings_list = [embeddings.embed_query(text) for text in texts]\n",
    "\n",
    "# Calculate cosine similarity\n",
    "def cosine_similarity(a, b):\n",
    "    dot_product = sum(x*y for x, y in zip(a, b))\n",
    "    magnitude_a = sum(x*x for x in a)**0.5\n",
    "    magnitude_b = sum(x*x for x in b)**0.5\n",
    "    return dot_product / (magnitude_a * magnitude_b)\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    for j in range(i+1, len(texts)):\n",
    "        similarity = cosine_similarity(embeddings_list[i], embeddings_list[j])\n",
    "        print(f\"  '{texts[i]}' vs '{texts[j]}': {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Step 6: Vector Database with ChromaDB\n",
    "\n",
    "A **Vector Database** stores and indexes embeddings for fast similarity search.\n",
    "\n",
    "### Why Vector Databases?\n",
    "- **Fast Search**: Find similar vectors in milliseconds\n",
    "- **Scalability**: Handle millions of documents\n",
    "- **Persistence**: Save embeddings to disk\n",
    "- **Filtering**: Search with metadata filters\n",
    "\n",
    "### ChromaDB Features:\n",
    "- Open-source and lightweight\n",
    "- Built-in embedding support\n",
    "- SQL-like filtering\n",
    "- Perfect for prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üóÑÔ∏è Creating vector database...\")\n",
    "\n",
    "# Create ChromaDB vector store\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,                    # Our text chunks\n",
    "    embedding=embeddings,                # Embedding function\n",
    "    persist_directory=\"./chroma_tutorial_db\",  # Save to disk\n",
    "    collection_name=\"langchain_tutorial\" # Collection name\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Vector store created with {len(chunks)} chunks\")\n",
    "\n",
    "# Test similarity search\n",
    "print(\"\\nüîç Testing similarity search:\")\n",
    "query = \"What is LangChain?\"\n",
    "similar_docs = vector_store.similarity_search(query, k=3)  # Get top 3 similar documents\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"Found {len(similar_docs)} similar documents:\\n\")\n",
    "\n",
    "for i, doc in enumerate(similar_docs):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(f\"  Source: {doc.metadata['source']}\")\n",
    "    print(f\"  Content: {doc.page_content[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## üîç Step 7: Understanding Retrieval\n",
    "\n",
    "**Retrieval** is the process of finding relevant documents for a given query.\n",
    "\n",
    "### Types of Retrieval:\n",
    "1. **Similarity Search**: Find most similar vectors\n",
    "2. **MMR (Maximal Marginal Relevance)**: Balance relevance and diversity\n",
    "3. **Threshold-based**: Only return results above similarity threshold\n",
    "\n",
    "### Key Parameters:\n",
    "- **k**: Number of documents to retrieve\n",
    "- **score_threshold**: Minimum similarity score\n",
    "- **fetch_k**: Number of docs to fetch before filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}  # Return top 3 most similar documents\n",
    ")\n",
    "\n",
    "print(\"üîç Testing different retrieval methods:\")\n",
    "\n",
    "queries = [\n",
    "    \"How do text splitters work?\",\n",
    "    \"What are embeddings?\",\n",
    "    \"Vector database benefits\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\n‚ùì Query: '{query}'\")\n",
    "    \n",
    "    # Method 1: Simple similarity search\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    print(f\"üìÑ Found {len(docs)} relevant documents\")\n",
    "    \n",
    "    # Show the most relevant result\n",
    "    if docs:\n",
    "        best_doc = docs[0]\n",
    "        print(f\"üèÜ Most relevant:\")\n",
    "        print(f\"   Source: {best_doc.metadata['source']}\")\n",
    "        print(f\"   Preview: {best_doc.page_content[:150]}...\")\n",
    "    \n",
    "    # Method 2: Search with similarity scores\n",
    "    docs_with_scores = vector_store.similarity_search_with_score(query, k=3)\n",
    "    print(f\"üìä Similarity scores:\")\n",
    "    for i, (doc, score) in enumerate(docs_with_scores):\n",
    "        print(f\"   #{i+1}: {score:.3f} - {doc.page_content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## ü§ñ Step 8: Language Model Setup\n",
    "\n",
    "The **Language Model (LLM)** generates human-like responses based on the retrieved context.\n",
    "\n",
    "### Our Setup:\n",
    "- **Model**: Meta Llama 3.1 8B Instruct\n",
    "- **Hosted on**: E2E Networks\n",
    "- **API**: OpenAI-compatible interface\n",
    "\n",
    "### Key Parameters:\n",
    "- **temperature**: Controls randomness (0 = deterministic, 1 = creative)\n",
    "- **max_tokens**: Maximum response length\n",
    "- **top_p**: Controls diversity of word choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the language model\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=E2E_API_KEY,\n",
    "    openai_api_base=E2E_LLM_ENDPOINT,\n",
    "    model_name=E2E_MODEL_NAME,\n",
    "    temperature=0.7,  # Balanced creativity\n",
    "    max_tokens=512    # Reasonable response length\n",
    ")\n",
    "\n",
    "print(f\"ü§ñ Language model initialized: {E2E_MODEL_NAME}\")\n",
    "\n",
    "# Test the LLM\n",
    "print(\"\\nüß™ Testing language model:\")\n",
    "test_prompt = \"Explain what a vector database is in simple terms.\"\n",
    "response = llm.invoke(test_prompt)\n",
    "\n",
    "print(f\"üí¨ Prompt: {test_prompt}\")\n",
    "print(f\"ü§ñ Response: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## üîó Step 9: Creating the RAG Chain\n",
    "\n",
    "Now we combine all components into a **RAG Chain**:\n",
    "\n",
    "### RAG Process:\n",
    "1. **User asks a question**\n",
    "2. **Retrieve relevant documents** from vector database\n",
    "3. **Create context** by combining retrieved documents\n",
    "4. **Generate prompt** with context + question\n",
    "5. **LLM generates answer** based on context\n",
    "6. **Return answer** to user\n",
    "\n",
    "### Benefits:\n",
    "- **Factual Accuracy**: Answers based on specific documents\n",
    "- **Source Attribution**: Can cite where information came from\n",
    "- **Up-to-date**: Add new documents without retraining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template for RAG\n",
    "rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant that answers questions based on the provided context.\n",
    "\n",
    "Context from LangChain documentation:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide a clear and accurate answer based on the context above. If the context doesn't contain enough information to answer the question, say so.\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "def rag_chain(question):\n",
    "    \"\"\"Complete RAG pipeline\"\"\"\n",
    "    \n",
    "    # Step 1: Retrieve relevant documents\n",
    "    print(f\"üîç Searching for: '{question}'\")\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    print(f\"üìÑ Found {len(docs)} relevant documents\")\n",
    "    \n",
    "    # Step 2: Create context from documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    print(f\"üìù Context length: {len(context)} characters\")\n",
    "    \n",
    "    # Step 3: Format the prompt\n",
    "    formatted_prompt = rag_prompt.format(context=context, question=question)\n",
    "    \n",
    "    # Step 4: Generate response\n",
    "    print(\"ü§ñ Generating response...\")\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    # Step 5: Return answer with sources\n",
    "    sources = [doc.metadata['source'] for doc in docs]\n",
    "    \n",
    "    return {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'num_docs': len(docs)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ RAG chain created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## üéØ Step 10: Testing the Complete RAG System\n",
    "\n",
    "Let's test our RAG system with various questions to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test questions about LangChain\n",
    "test_questions = [\n",
    "    \"What is LangChain and what is it used for?\",\n",
    "    \"How do text splitters work in LangChain?\",\n",
    "    \"What are the benefits of using vector databases?\",\n",
    "    \"How do embeddings help in document search?\"\n",
    "]\n",
    "\n",
    "print(\"üéØ Testing RAG System:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\nüìù Test {i}: {question}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Get RAG response\n",
    "    result = rag_chain(question)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nü§ñ Answer:\")\n",
    "    print(result['answer'])\n",
    "    \n",
    "    print(f\"\\nüìö Sources ({result['num_docs']} documents):\")\n",
    "    for j, source in enumerate(result['sources'], 1):\n",
    "        print(f\"  {j}. {source}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## üîß Step 11: Advanced RAG Features\n",
    "\n",
    "Let's explore some advanced features to improve our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Retrieval: MMR (Maximal Marginal Relevance)\n",
    "# This balances relevance with diversity to avoid redundant results\n",
    "\n",
    "mmr_retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 3,           # Final number of documents\n",
    "        \"fetch_k\": 10,    # Fetch more docs, then filter for diversity\n",
    "        \"lambda_mult\": 0.7  # Balance: 0=max diversity, 1=max relevance\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"üîß Testing MMR retrieval:\")\n",
    "query = \"What are the benefits of vector databases?\"\n",
    "\n",
    "# Compare standard vs MMR retrieval\n",
    "standard_docs = retriever.get_relevant_documents(query)\n",
    "mmr_docs = mmr_retriever.get_relevant_documents(query)\n",
    "\n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "print(f\"\\nüìä Standard Retrieval (top 3):\")\n",
    "for i, doc in enumerate(standard_docs):\n",
    "    print(f\"  {i+1}. {doc.page_content[:100]}...\")\n",
    "\n",
    "print(f\"\\nüéØ MMR Retrieval (diverse top 3):\")\n",
    "for i, doc in enumerate(mmr_docs):\n",
    "    print(f\"  {i+1}. {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": "# Metadata Filtering\n# Search within specific sources only\n\nprint(\"üîç Testing metadata filtering:\")\n\n# Filter by specific source\nfiltered_docs = vector_store.similarity_search(\n    \"embeddings\",\n    k=5,\n    filter={\"source\": \"https://python.langchain.com/docs/concepts/embedding_models/\"}\n)\n\nprint(f\"\\nüìÑ Documents about embeddings from specific source:\")\nfor i, doc in enumerate(filtered_docs):\n    print(f\"  {i+1}. Source: {doc.metadata['source']}\")\n    print(f\"     Preview: {doc.page_content[:150]}...\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced RAG with confidence scoring\n",
    "\n",
    "def enhanced_rag_chain(question, use_mmr=True, min_score=0.7):\n",
    "    \"\"\"Enhanced RAG with confidence scoring\"\"\"\n",
    "    \n",
    "    # Choose retrieval method\n",
    "    if use_mmr:\n",
    "        docs = mmr_retriever.get_relevant_documents(question)\n",
    "        retrieval_method = \"MMR\"\n",
    "    else:\n",
    "        docs = retriever.get_relevant_documents(question)\n",
    "        retrieval_method = \"Standard\"\n",
    "    \n",
    "    # Get similarity scores\n",
    "    docs_with_scores = vector_store.similarity_search_with_score(question, k=len(docs))\n",
    "    \n",
    "    # Filter by minimum score\n",
    "    high_confidence_docs = [(doc, score) for doc, score in docs_with_scores if score >= min_score]\n",
    "    \n",
    "    if not high_confidence_docs:\n",
    "        return {\n",
    "            'answer': \"I don't have enough confident information to answer this question.\",\n",
    "            'sources': [],\n",
    "            'confidence': 'Low',\n",
    "            'retrieval_method': retrieval_method\n",
    "        }\n",
    "    \n",
    "    # Use only high-confidence documents\n",
    "    confident_docs = [doc for doc, score in high_confidence_docs]\n",
    "    avg_score = sum(score for doc, score in high_confidence_docs) / len(high_confidence_docs)\n",
    "    \n",
    "    # Create context\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in confident_docs])\n",
    "    \n",
    "    # Enhanced prompt with confidence indication\n",
    "    enhanced_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    You are a helpful assistant answering questions based on LangChain documentation.\n",
    "    \n",
    "    Context (confidence score: {confidence:.2f}):\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Provide a clear answer based on the context. If you're uncertain about any part of your answer, mention it.\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\")\n",
    "    \n",
    "    formatted_prompt = enhanced_prompt.format(\n",
    "        context=context, \n",
    "        question=question, \n",
    "        confidence=avg_score\n",
    "    )\n",
    "    \n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    # Determine confidence level\n",
    "    if avg_score >= 0.9:\n",
    "        confidence = \"Very High\"\n",
    "    elif avg_score >= 0.8:\n",
    "        confidence = \"High\"\n",
    "    elif avg_score >= 0.7:\n",
    "        confidence = \"Medium\"\n",
    "    else:\n",
    "        confidence = \"Low\"\n",
    "    \n",
    "    return {\n",
    "        'answer': response.content,\n",
    "        'sources': [doc.metadata['source'] for doc in confident_docs],\n",
    "        'confidence': confidence,\n",
    "        'avg_score': avg_score,\n",
    "        'num_docs': len(confident_docs),\n",
    "        'retrieval_method': retrieval_method\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Enhanced RAG chain created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test enhanced RAG\n",
    "print(\"üöÄ Testing Enhanced RAG System:\\n\")\n",
    "\n",
    "enhanced_questions = [\n",
    "    \"What is LangChain?\",  # Should have high confidence\n",
    "    \"How do I cook pasta?\",  # Should have low confidence (not in our docs)\n",
    "]\n",
    "\n",
    "for question in enhanced_questions:\n",
    "    print(f\"‚ùì Question: {question}\")\n",
    "    result = enhanced_rag_chain(question)\n",
    "    \n",
    "    print(f\"üéØ Confidence: {result['confidence']} (Score: {result.get('avg_score', 0):.3f})\")\n",
    "    print(f\"üìä Retrieval: {result['retrieval_method']} | Documents: {result['num_docs']}\")\n",
    "    print(f\"ü§ñ Answer: {result['answer'][:200]}...\")\n",
    "    print(f\"üìö Sources: {len(result['sources'])} documents\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## üìä Step 12: RAG System Evaluation\n",
    "\n",
    "Let's analyze how well our RAG system performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics and analysis\n",
    "\n",
    "def evaluate_retrieval(questions):\n",
    "    \"\"\"Evaluate retrieval quality\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for question in questions:\n",
    "        docs_with_scores = vector_store.similarity_search_with_score(question, k=5)\n",
    "        scores = [score for doc, score in docs_with_scores]\n",
    "        \n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'top_score': max(scores) if scores else 0,\n",
    "            'avg_score': sum(scores) / len(scores) if scores else 0,\n",
    "            'score_range': max(scores) - min(scores) if scores else 0\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test questions for evaluation\n",
    "eval_questions = [\n",
    "    \"What is LangChain?\",\n",
    "    \"How do embeddings work?\",\n",
    "    \"What are vector databases?\",\n",
    "    \"How to split text documents?\",\n",
    "    \"What is retrieval augmented generation?\"\n",
    "]\n",
    "\n",
    "eval_results = evaluate_retrieval(eval_questions)\n",
    "\n",
    "print(\"üìä Retrieval Quality Analysis:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for result in eval_results:\n",
    "    print(f\"‚ùì {result['question']}\")\n",
    "    print(f\"   Top Score: {result['top_score']:.3f}\")\n",
    "    print(f\"   Avg Score: {result['avg_score']:.3f}\")\n",
    "    print(f\"   Range: {result['score_range']:.3f}\")\n",
    "    print()\n",
    "\n",
    "# Overall statistics\n",
    "all_top_scores = [r['top_score'] for r in eval_results]\n",
    "all_avg_scores = [r['avg_score'] for r in eval_results]\n",
    "\n",
    "print(f\"üìà Overall Performance:\")\n",
    "print(f\"   Average Top Score: {sum(all_top_scores)/len(all_top_scores):.3f}\")\n",
    "print(f\"   Average Avg Score: {sum(all_avg_scores)/len(all_avg_scores):.3f}\")\n",
    "print(f\"   Questions with High Confidence (>0.8): {sum(1 for s in all_top_scores if s > 0.8)}/{len(all_top_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## üéì Step 13: Key Takeaways and Best Practices\n",
    "\n",
    "### What We've Learned:\n",
    "\n",
    "#### üèóÔ∏è **RAG Architecture**\n",
    "- **Document Loading**: Web scraping ‚Üí Clean text extraction\n",
    "- **Text Processing**: Chunking with overlap for context preservation\n",
    "- **Embeddings**: Convert text to semantic vectors\n",
    "- **Vector Storage**: Fast similarity search with ChromaDB\n",
    "- **Retrieval**: Find relevant context for queries\n",
    "- **Generation**: LLM creates answers from context\n",
    "\n",
    "#### ‚ö° **Performance Optimizations**\n",
    "- **Chunk Size**: Balance between context and specificity\n",
    "- **Overlap**: Prevents important information from being split\n",
    "- **Retrieval Methods**: MMR for diversity, similarity for relevance\n",
    "- **Confidence Scoring**: Filter low-quality matches\n",
    "\n",
    "#### üõ†Ô∏è **Best Practices**\n",
    "1. **Start Simple**: Basic similarity search, then add complexity\n",
    "2. **Measure Performance**: Track retrieval scores and answer quality\n",
    "3. **Iterate on Chunks**: Experiment with sizes and overlap\n",
    "4. **Source Attribution**: Always provide document sources\n",
    "5. **Handle Edge Cases**: What if no good matches are found?\n",
    "\n",
    "#### üöÄ **Next Steps**\n",
    "- **Add More Documents**: Expand your knowledge base\n",
    "- **Fine-tune Retrieval**: Adjust parameters based on your data\n",
    "- **Add Memory**: For conversational RAG\n",
    "- **Implement Caching**: For faster repeated queries\n",
    "- **Add UI**: Build a chat interface like Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final interactive demo\n",
    "print(\"üéØ Interactive RAG Demo - Ask your own questions!\")\n",
    "print(\"(Enter 'quit' to exit)\\n\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_question = input(\"‚ùì Your question: \").strip()\n",
    "        \n",
    "        if user_question.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"üëã Thanks for using the RAG tutorial!\")\n",
    "            break\n",
    "        \n",
    "        if not user_question:\n",
    "            continue\n",
    "        \n",
    "        # Get enhanced RAG response\n",
    "        result = enhanced_rag_chain(user_question)\n",
    "        \n",
    "        print(f\"\\nü§ñ Answer (Confidence: {result['confidence']}):\")\n",
    "        print(result['answer'])\n",
    "        \n",
    "        if result['sources']:\n",
    "            print(f\"\\nüìö Sources:\")\n",
    "            for i, source in enumerate(result['sources'], 1):\n",
    "                print(f\"  {i}. {source}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nüëã Goodbye!\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        print(\"Please try again.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully built a complete RAG system from scratch! \n",
    "\n",
    "### What You've Accomplished:\n",
    "- ‚úÖ **Document Loading** from websites\n",
    "- ‚úÖ **Text Splitting** with smart chunking\n",
    "- ‚úÖ **Embedding Creation** for semantic search\n",
    "- ‚úÖ **Vector Database** setup with ChromaDB\n",
    "- ‚úÖ **Retrieval System** with multiple strategies\n",
    "- ‚úÖ **LLM Integration** with E2E Networks\n",
    "- ‚úÖ **Complete RAG Pipeline** with confidence scoring\n",
    "- ‚úÖ **Performance Evaluation** and optimization\n",
    "\n",
    "### Ready for Production?\n",
    "Your RAG system is now ready to be integrated into applications like:\n",
    "- **Chatbots** for customer support\n",
    "- **Documentation assistants** for internal tools\n",
    "- **Research tools** for academic work\n",
    "- **Knowledge management** systems\n",
    "\n",
    "Happy building! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}